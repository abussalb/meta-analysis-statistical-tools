% adding the line below for Multifile document support with LatexTools Sublime package 
%!TEX root = manuscript.tex


% Discussion

\section{Discussion}

\subsection{Meta-analysis} 

\textcolor{red}{This replication and update of a meta-analysis did not meet all PRISMA recommendations \citep{Moher2009}. In particular, the risk of bias
in individual and across studies was not assessed.}  

In the meta-analysis performed here, we challenged some choices made by \citeauthor{Cortese2016}, which proved controversial: 
the computation of between-\gls{es} based on an unusual scale \citep{Steiner2014} and the inclusion of a pilot study \citep{Arnold2014} 
whose endpoint values were not available at the time \citeauthor{Cortese2016} conducted their meta-analysis. We here review the 
list of changes, their justification, and their impact on the analysis.
 
First, relying on the Conners-3 \citep{Conners2011} instead of the BOSS Classroom Observation \citep{Shapiro2010} for
teachers' ratings seems preferable because this scale is more commonly used \citep{Christiansen2014, Bluschke2016} and is
a revision of the Conners Rating Scale Revised \citep{Conners1998} whose reliability has been studied \citep{Collett2003}. 
However, relying on one or the other scale did not change the significance of the between-\gls{es}, regardless of outcome.

Second, to compute the between-\gls{es} of \citet{Arnold2014} the clinical scores taken when all sessions were completed were 
used instead of looking at interim results as with \citeauthor{Cortese2016}. Some studies suggested that the number of sessions 
correlates positively with the changes observed in the \gls{eeg} \citep{Vernon2004} so that a lower number of sessions would 
lead to artificially smaller between-\gls{es}. Here, the between-\gls{es} computed with the values at post test of \citet{Arnold2014} were smaller 
than those obtained after 12 sessions; however, these differences did not lead to a change of significance of the \gls{se}. 

To conclude on this meta-analysis, although some points were controversial, the impact on the
meta-analysis was minimal and did not change the statistical significance of any outcome. 
The addition of the two new studies \citep{Strehl2017, Baumeister2016} further confirmed the original results. Indeed, the
significance did not change for any outcome: the \gls{se} remained significant for \gls{mprox} raters and
non-significant for \gls{pblind}. Adding two more studies increased the significance of the sensitivity analysis run by
\citeauthor{Cortese2016}, most notably the \gls{se} of studies corresponding to \gls{nfb} "standard protocols" \citep{Arns2014}. 
While \citeauthor{Cortese2016} found that this subset tends to perform better, particularly on the \gls{pblind} outcome, 
adding two studies also extended this result to the total clinical score as well (p-value < 0.05). Despite the obvious heterogeneity 
of the studies included in this subset (particularly in terms of protocol used), these results suggest a positive relation 
between the features of this \emph{standard} design and \gls{nfb} performance. \textcolor{red}{This result is a breakthrough in the demonstration 
of standard \gls{nfb} protocol efficacy for the treatment of \gls{adhd}. Nonetheless, the  studies 
included in this subset are still highly heterogeneous (particularly in terms of protocol used), a factor which should be accounted for.}


\subsection{Factors influencing neurofeedback}

Description and analysis of different types of \gls{nfb} implementation were subject to several studies \citep{Arns2014, 
Enriquez2017, Vernon2004, Jeunet2018}. However, to the best of our knowledge none used statistical tools to quantify their influence on
clinical endpoints. 

Surprisingly, the number of sessions was not found to be significant by any method, which was somewhat
in contradiction with existing literature. For instance, \citet{Arns2014} stated that performing less than
20 \gls{nfb} sessions leads to smaller effects. Similarly, \citet{Vernon2004} observed that positive changes in the \gls{eeg}
and behavioral performance occurred after a minimum of 20 sessions. However, \citet{Enriquez2017} insisted that the number of
sessions should be carefully chosen in order to avoid "overtraining". The fact that the number of sessions was not identified as a 
positively contributing factor might be explained by the presence of only one data point with 20 sessions or less. Conceivably, 
the temporal threshold of efficacy was passed for all included studies, making the identification of this factor unlikely on 
this dataset. However, regardless of its statistical significance, the coefficient found by the \gls{wls} was negative, meaning 
that, as expected, the more sessions performed, the more efficient the \gls{nfb} tends to be. 

Interestingly, \citep{Minder2018} suggests that the subject location of the \gls{nfb} training may also be an important contributory 
factor to clinical effectiveness. However, this has been challenged by a recent study \citep{Minder2018} showing that 
performing \gls{nfb} at school or at the clinic has no significant impact on treatment response. 

The type of \gls{nfb} protocol was not identified by more than one method, and did not appear to influence the \gls{nfb} results. 
This minimal importance granted by the methods to the \gls{nfb} protocols is counter-intuitive given the centrality of the 
protocols in the neurophysiological mode of action and subsequent expected impact on
therapeutic effectiveness \citep{Vernon2004}. A possible explanation for this result is that these protocols were equally 
efficacious for the populations to whom they were offered and thereby did not constitute a significant explanatory factor. 
This result, however, does not preclude a combined and personalized strategy (offering personalized protocols based on phenotypes)
to further improve performance, as previously suggested by \citet{Alkoby2017}.

Several factors were selected by all three methods with the same direction of influence: the EEG quality, the treatment 
length, and the rater's probably blindness to the treatment. First, our analysis highlighted that recording \gls{eeg} 
in good conditions leads to better results.
This can be explained by the fact that better signal quality enables more accurate extraction of \gls{eeg} patterns
linked to \gls{adhd} and hence leads to better learning and therapeutic efficacy \citep{Congedo2004}. 
However, it remains difficult to assess the quality of \gls{eeg} hardware (such as the amplifier used) 
because little information is provided in these studies.  
This calls for greater care in future studies, which should strive to assess and report the quality of the data.

Next, it appears that the longer the course of treatment, the less efficient it becomes. Arguably, the treatment length is a
proxy for treatment intensity, suggesting that a shorter period of treatment is more likely to succeed because the frequency of the sessions
is higher. This hypothesis is supported by the fact that the variable \emph{session pace} (number of
sessions per week) is also associated with larger within-\gls{es} according to the \gls{wls} and \gls{lasso}. The impact of the
intensity of treatment has been investigated by \citep{Rogala2016} on healthy adults: it was observed that studies with
at least four training sessions completed on consecutive days were all beneficial. Overall, these results suggest adopting a high session pace, 
which is not common knowledge in the field.

\textcolor{red}{In general our results strongly support the effectiveness of \gls{nfb} for the treatment of \gls{adhd}. However, as expected, the assessment of symptoms by non-blind raters 
leads to far more favorable results than by \gls{pblind} raters, a result widely expected and in close compliance with the existing 
meta-analysis \citep{Cortese2016, Micoulaud2014}. This observation would certainly be contradictory should teachers’ 
assessments reflect a placebo effect, which has long been documented in the literature \citep{Sollie2013, Narad2015, Minder2018}. 
This point is investigated in greater detail in the following section.}

\subsection{Analysis on the probably blind raters}

Teachers were considered as \gls{pblind} raters by \citeauthor{Cortese2016} and \citeauthor{Micoulaud2014}.
Unexpectedly, the data provided did not exactly match the widely accepted hypothesis stating that the difference between
\gls{mprox} and \gls{pblind} can solely be explained by the placebo effect. 
Nonetheless, the emphasis put on 'probably' indicates that teachers may be aware of the treatment followed. 
An element that corroborates this hypothesis is the fact that, for all the studies included in this work, the amplitude 
of the clinical scale at baseline suggests that teachers did not capture the full extent of the symptoms or, put differently, 
that they were blind more to the symptoms than to the intervention, as illustrated 
in Figure~\ref{Figure:discussion_on_placebo_effect_colors_2-columns_fitting_image}. 
\textcolor{red}{Indeed, before the intervention, teachers rated the symptoms less severely compared to parents and observed less improvement at post-test: 
this tends to correspond more to case A with no placebo effect than case B}. The expected differences of ratings between 
teachers and parents have been extensively studied \citep{Sollie2013, Narad2015, Minder2018}, observing that teachers are more 
likely to underrate a child's symptom severity, especially for younger children. As a consequence, teachers might simply be less likely 
to observe a clinical change over the course of the treatment \citep{Sollie2013, Narad2015, Minder2018}. Moreover, it is also clear 
that there is more variability in teachers' scores compared to parents', which could partly explain the lower \gls{es} obtained for 
\gls{pblind} raters, since the variability deflates the \gls{es}. In conclusion, using \gls{pblind} as an estimate for correcting the 
placebo effect does not appear an appropriate choice. 

Another way to highlight a possible placebo effect is to focus on the decision tree illustrated in Figure
~\ref{Figure:factors_analysis_decision_tree_results}.
The top node splits: on the one hand 45 observations corresponding to \gls{mprox} raters and, on the other, 
21 observations corresponding to \gls{pblind}. If the differences observed between \gls{pblind} and \gls{mprox} raters were 
due to the placebo effect, one would expect to find in the \gls{mprox} sub-tree some factors linked to the perception
of the implication in the treatment. This was indeed the case: session and treatment length were found to be significant but not in the
direction corroborating the hypothesis that they are a part of a placebo effect. Indeed, one would expect that the
longer the session and the treatment, the higher the placebo effect and the greater the within-\gls{es}. Instead, the opposite was found, 
somewhat invalidating the hypothesis. 

Overall, these results suggest that \gls{pblind} assessments could hardly be used to assess placebo effect as they seem to be blinder 
to symptoms than to intervention. In the absence of an ethically \citep{Holtmann2014} and technically \citep{Birbaumer1991} feasible sham 
for \gls{nfb} protocols \citep{World-Medical-Association2000}, it is necessary to fall back on an acceptable methodological alternative for 
the demonstration of clinical effectiveness. Among those are the analysis of neuromarkers collected during \gls{nfb} treatment demonstrating 
that patients do \emph{control} the trained neuromarkers; that they \emph{learn} (reinforce control over time), and that these possibly 
lead to lasting brain reorganization (e.g., changes in their baseline resting state activity). The specificity of these changes, in relation 
to which neuromarkers were trained and to the clinical improvement, will be an essential component of this demonstration.  

% words number = 1506
