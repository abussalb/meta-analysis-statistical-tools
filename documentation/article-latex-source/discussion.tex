% adding the line below for Multifile document support with LatexTools Sublime package 
%!TEX root = manuscript.tex


% Discussion

\section{Discussion}

\subsection{Meta-analysis} 

In the meta-analysis performed here, we challenged some choices made by \citeauthor{Cortese2016}, which proved controversial: 
the computation of \gls{es} based on an unusual scale \citep{Steiner2014} and the inclusion of a pilot study \citep{Arnold2014} 
whose end point values were not available at the time \citeauthor{Cortese2016} conducted his meta-analysis. We review here the 
list of changes, their justification, and their impact on the analysis.
 
First, relying on the Conners-3 \citep{Conners2011} instead of the BOSS Classroom Observation \citep{Shapiro2010} for
teachers ratings seemed preferable because this scale is more commonly used \citep{Christiansen2014, Bluschke2016} and is
the revision of the Conners Rating Scale Revised \citep{Conners1998} whose reliability has been studied \citep{Collett2003}. 
However, relying on one or the other scale did not change the significance of the \gls{es}, regardless the outcome.

Second, to compute the \gls{es} of \citet{Arnold2014} the clinical scores taken when all sessions were completed were 
used instead of looking at interim results as in \citeauthor{Cortese2016}. Some studies suggested that the number of sessions 
correlates positively with the changes observed in the \gls{eeg} \citep{Vernon2004} so that a lower number of sessions would 
lead to artificially smaller \gls{es}. Here, the \gls{es} computed with the values at post test of \citet{Arnold2014} were smaller 
than those obtained after 12 sessions but these differences did not lead to a change of significance of the \gls{se}. 

To conclude on that meta-analysis, though some points from the original were controversial and the fact that - for the aforementioned 
reasons - different choices could reasonably be made, it turned out that the impact on the
meta-analysis results were minimal and did not change the statistical significance of any outcome. Consequently, the
completion of the meta-analysis with studies published since the publication of his work were done:
\begin{itemize} 
  \item \gls{es} of \citet{Arnold2014} computed with final post-values; 
  \item the scores reported by teachers on the Conners-3 in \citeauthor{Steiner2014}
\end{itemize} 

The addition of the two new studies \citep{Strehl2017, Baumeister2016} further confirmed original results. Indeed, the
significance did not change for any outcome: the \gls{se} remained significant for \gls{mprox} raters and
non-significant for \gls{pblind}. Adding two more studies increased the significance of the sensitivity analysis ran by
\citeauthor{Cortese2016}, most notably, the \gls{se} of studies corresponding to \gls{nfb} "`standard protocols"' \citep{Arns2014}. 
While \citeauthor{Cortese2016} found that this subset tended to perform better, particularly on the \gls{pblind} outcome, 
adding two studies extended this result on the total clinical score as well (p-value < 0.05). Despite the obvious heterogeneity 
of the studies included in this subset (particularly in terms of protocol used), these results suggest a positive relation 
between the features of this \emph{standard} design and \gls{nfb} performance.

This replication and update of a meta-analysis did not meet all PRISMA recommendations. In particular, the risk of bias
in individual and across studies was not assessed.  

\subsection{Factors influencing Neurofeedback}

Description and analysis of different types of \gls{nfb} implementation was subject to several studies \citep{Arns2014, 
Enriquez2017, Vernon2004, Jeunet2018} but to our knowledge none used statistical tools to quantify their influence on
clinical endpoints. 

Surprisingly, the number of sessions factor was not found significant by any method, which was somewhat
in contradiction with existing literature. For instance, \citet{Arns2014} stated that performing less than
20 \gls{nfb} sessions led to smaller effects. Similarly, \citet{Vernon2004} observed that positive changes in the \gls{eeg}
and behavioral performance occurred after a minimum of 20 sessions. However, \citet{Enriquez2017} insisted on the fact that the number of
sessions should be chosen carefully in order to avoid "overtraining". The fact that the number of sessions was not identified as a 
positively contributing factor, might be explained by the presence of only one data point with 20 sessions or less. Possibly, 
the temporal threshold of efficacy was passed for all included studies making the identification of this factor unlikely on 
this dataset. However, regardless of its statistical significance, the coefficient found by the \gls{wls} was negative, meaning 
that, as expected, the more sessions are performed, the more efficient the \gls{nfb} seemed to be. 

Interestingly, \citep{Minder2018} suggests that the subject location of the \gls{nfb} training may also be an important contributing 
factor to clinical effectiveness. This was however recently challenged by a recent study \citep{Minder2018} showing that 
performing \gls{nfb} at school or at the clinic has no significant impact on treatment response. 

The type of \gls{nfb} protocol was not identified by all the three methods, but it seemed to influence the \gls{nfb} results
according to two methods. In particular, the theta down protocol appeared more efficient and the \gls{smr} protocol. 
This importance granted by the methods to the \gls{nfb} protocols was somewhat lower to
expectations given their centrality in the neurophysiological mode of action and subsequent expected impact on
therapeutic effectiveness \citep{Vernon2004}. A possible explanation for this result is that these protocols were equally 
efficacious to the populations they were offered to and thereby did not constitute a significant explanatory factor. 
This result, however, does not preclude a combined and personalized strategy (offering the right protocol to the right kid) 
to further improve performance, as previously suggested by \citet{Alkoby2017}.

Several factors were selected by all three methods with the same direction of influence: the EEG quality, the treatment 
length, and the rater's probably blindness to the treatment.

First, this analysis pointed out the fact that recording \gls{eeg} in good conditions seems to lead to better results.
This can be explained by the fact that better signal quality enables to extract the \gls{eeg} patterns
linked to \gls{adhd} more correctly and henceforth leads to better learning and therapeutic efficacy \citep{Congedo2004}. 
However, it remains difficult to assess the quality of \gls{eeg} hardware (such as the amplifier used) 
because little information is provided in these studies.  
This calls for a greater care in the future studies, which should strive and to assess and report the quality of the data.

Next, it appears here that the longer the treatment, the less efficient it becomes. Arguably, the treatment length is a
proxy for treatment intensity, suggesting that a shorter treatment is more likely to succeed because the frequency of the sessions
is higher. This hypothesis was back-up by the fact that the variable \emph{session pace} (number of
sessions per week) is also associated with larger \gls{es} according to the \gls{wls} and \gls{lasso}. Impact of the
intensity of treatment have been investigated by \citep{Rogala2016} on healthy adults: it was observed that studies with
at least four training sessions completed on consecutive days were all successful. 

As expected, the assessment of symptoms by non-blind raters leads to far more favorable results than by \gls{pblind} raters -
result widely expected and in close compliance with existing meta-analyses \citep{Cortese2016, Micoulaud2014}. This last point
was investigated more precisely to determine if this observation could solely be explained by the placebo effect. 

\subsection{Analysis on the probably blind raters}

Teachers were considered as \gls{pblind} raters by \citeauthor{Cortese2016} and \citeauthor{Micoulaud2014}.
Unexpectedly, the data provided did not exactly matched the widely accepted hypothesis stating that the difference between
\gls{mprox} and \gls{pblind} can solely be explained by the placebo effect. 
Nonetheless, the stress put on \emph{probably} indicated that teachers may be aware of the treatment followed. 
An element that corroborates this hypothesis is the fact that for all the studies included in this work the amplitude 
of the clinical scale at baseline suggests that teachers did not capture the full extent of the symptoms or, said differently, 
that they were more blind to the symptoms than to the intervention as illustrated 
in Figure~\ref{Figure:discussion_on_placebo_effect_colors_2-columns_fitting_image}. The expected differences of ratings between 
teachers and parents have been extensively studied \citep{Sollie2013, Narad2015, Minder2018}, noting that teachers are more 
likely to underrate a child's symptom severity, especially so for younger children. As a consequence, teachers might just 
be less likely to observe a clinical change over the course of the treatment (prone to type II error) 
\citep{Sollie2013, Narad2015, Minder2018}. Besides, it is also clear that there is more variability in teachers' scores compared 
to parents', which could partly explain the lower \gls{es} obtained for \gls{pblind} raters, since the variability deflates
the \gls{es}. In conclusion, using \gls{pblind} as an estimate for correcting the placebo effect does not appear 
an adequate choice. 

Another way to highlight a possible placebo effect is to focus on the decision tree illustrated in Figure
~\ref{Figure:factors_analysis_decision_tree_results}, whose results provide a good insight to comment on this.
The top node splits: on one hand 43 observations corresponding to \gls{mprox} raters and, on the other hand, 
19 observations corresponding to \gls{pblind}. If the differences observed between \gls{pblind} and \gls{mprox} raters were 
due to the placebo effect, one would expect to find in the \gls{mprox} sub-tree some factors linked to the perception
of the implication in the treatment. It was, indeed the case: session and treatment length were found significant but not in the
direction corroborating the hypothesis that they are as part of a placebo effect. Indeed, one would expect that the
longer the session and the treatment, the higher the placebo effect and the larger the within-\gls{es}. Instead, the opposite was found, 
somewhat invalidating the hypothesis. 

These results altogether suggest that \gls{pblind} assessments could hardly be used to assess placebo effect as they seem to be blinder 
to symptoms than to intervention. In the absence of ethically \citep{Holtmann2014} and technically \citep{Birbaumer1991} feasible sham 
for \gls{nfb} protocols \citep{World-Medical-Association2000}, it is necessary to fall back on acceptable methodological alternative for 
the demonstration of clinical effectiveness. Among those are the analysis of neuromarkers collected during \gls{nfb}Â treatment demonstrating 
that patients do \emph{control} the trained neuromarkers; that they \emph{learn} (reinforce control over time), and that these possibly 
lead to lasting brain reorganization (e.g., changes in their baseline resting state activity). The specificity of these changes, in relation 
to, which neuromarkers were trained and to the clinical improvement will be an essential component of this demonstration.  

% words number = 1674
